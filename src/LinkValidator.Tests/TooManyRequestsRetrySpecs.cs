// -----------------------------------------------------------------------
// <copyright file="TooManyRequestsRetrySpecs.cs">
//      Copyright (C) 2025 - 2025 Aaron Stannard <https://aaronstannard.com/>
// </copyright>
// -----------------------------------------------------------------------

using System.Collections.Concurrent;
using System.Net;
using Akka.Actor;
using Akka.TestKit.Xunit2;
using LinkValidator.Actors;
using LinkValidator.Util;
using Microsoft.AspNetCore.Builder;
using Microsoft.AspNetCore.Hosting;
using Microsoft.AspNetCore.Http;
using Microsoft.Extensions.Hosting;
using Xunit.Abstractions;
using static LinkValidator.Util.CrawlerHelper;
using static LinkValidator.Util.MarkdownHelper;

namespace LinkValidator.Tests;

public class TooManyRequestsRetrySpecs : TestKit, IClassFixture<TestWebServerFixture>
{
    private readonly TestWebServerFixture _webServerFixture;
    private readonly ITestOutputHelper _output;

    public TooManyRequestsRetrySpecs(ITestOutputHelper output, TestWebServerFixture webServerFixture) : base(output: output)
    {
        _webServerFixture = webServerFixture;
        _output = output;
        _webServerFixture.Logger = _output.WriteLine;
    }

    [Fact]
    public async Task ShouldRetryExternalLinksAndGenerateCorrectReport()
    {
        // arrange - create test pages with external links that will return 429
        var testPagesDir = Path.Join(Directory.GetCurrentDirectory(), "test-pages-retry");
        Directory.CreateDirectory(testPagesDir);
        
        // Track retry attempts for our mock server
        var retryTracker = new RetryAttemptTracker();
        
        try
        {
            // Create test pages
            var indexPageContent = """
                <html>
                    <body>
                        <h1>Test Page with Rate Limited Links</h1>
                        <p>This page contains links that will initially return 429.</p>
                        <a href="/about.html">About Page</a>
                        <a href="http://127.0.0.1:8082/always-429">Always Rate Limited</a>
                        <a href="http://127.0.0.1:8082/retry-then-succeed">Eventually Succeeds</a>
                        <a href="http://127.0.0.1:8082/with-retry-after">With Retry-After Header</a>
                    </body>
                </html>
                """;
                
            var aboutPageContent = """
                <html>
                    <body>
                        <h1>About Page</h1>
                        <p>Internal page that links to more external resources.</p>
                        <a href="http://127.0.0.1:8082/another-rate-limited">Another Rate Limited Link</a>
                    </body>
                </html>
                """;
                
            File.WriteAllText(Path.Join(testPagesDir, "index.html"), indexPageContent);
            File.WriteAllText(Path.Join(testPagesDir, "about.html"), aboutPageContent);
            
            // Start main test server
            _webServerFixture.StartServer(testPagesDir, 8081);
            var baseUrl = new AbsoluteUri(new Uri(_webServerFixture.BaseUrl!));
            
            // Start mock server for external links that simulates 429 responses
            using var mockServer = CreateMockRateLimitedServer(retryTracker, 8082);
            await mockServer.StartAsync();
            
            // Configure for multiple retries with short delays for testing
            var crawlSettings = new CrawlConfiguration(
                baseUrl, 
                5, 
                TimeSpan.FromSeconds(5), // Reasonable timeout
                3,  // 3 retry attempts 
                TimeSpan.FromMilliseconds(200) // Very short retry delay for testing
            );
            
            _output.WriteLine("=== CRAWL CONFIGURATION ===");
            _output.WriteLine($"Max External Retries: {crawlSettings.MaxExternalRetries}");
            _output.WriteLine($"Default Retry Delay: {crawlSettings.DefaultExternalRetryDelay.TotalMilliseconds}ms");
            _output.WriteLine($"Request Timeout: {crawlSettings.RequestTimeout.TotalSeconds}s");
            
            // act
            var startTime = DateTime.UtcNow;
            var crawlResult = await CrawlWebsite(Sys, baseUrl, crawlSettings);
            var elapsed = DateTime.UtcNow - startTime;
            
            // Generate markdown report like End2EndSpecs
            var markdown = GenerateMarkdown(crawlResult);
            
            _output.WriteLine("=== CRAWL RESULTS ===");
            _output.WriteLine($"Crawl completed in {elapsed.TotalSeconds:F1} seconds");
            _output.WriteLine($"Found {crawlResult.InternalLinks.Count} internal links and {crawlResult.ExternalLinks.Count} external links");
            
            _output.WriteLine("=== RETRY ATTEMPT TRACKING ===");
            foreach (var (url, attempts) in retryTracker.GetAttemptCounts())
            {
                _output.WriteLine($"{url}: {attempts} attempts");
            }
            
            _output.WriteLine("=== RAW MARKDOWN OUTPUT ===");
            _output.WriteLine(markdown);
            _output.WriteLine("=== END RAW MARKDOWN ===");
            
            // assert
            Assert.NotNull(crawlResult);
            
            // Verify crawl completed in reasonable time (should be much faster than timeout due to retries)
            Assert.True(elapsed < TimeSpan.FromSeconds(30), $"Crawl took {elapsed.TotalSeconds:F1}s, should be much faster");
            
            // Should have found external links
            Assert.True(crawlResult.ExternalLinks.Count >= 3, $"Should have found at least 3 external links, found {crawlResult.ExternalLinks.Count}");
            
            // Verify retry attempts were made
            var attemptCounts = retryTracker.GetAttemptCounts();
            Assert.True(attemptCounts.Values.Any(count => count > 1), "Should have made retry attempts for some URLs");
            
            // Verify that retry attempts were made and crawl completed without hanging
            var externalLinkStatuses = crawlResult.ExternalLinks.Values.Select(r => r.StatusCode).ToList();
            _output.WriteLine($"External link statuses: {string.Join(", ", externalLinkStatuses)}");
            
            // Some should succeed after retries, some should fail after exhausting retries
            Assert.Contains(System.Net.HttpStatusCode.OK, externalLinkStatuses); // Should have some successful retries
            Assert.Contains(System.Net.HttpStatusCode.TooManyRequests, externalLinkStatuses); // Should have some failed after retries
            
            // Verify markdown report includes the retry information
            Assert.Contains("ðŸ”´ Pages with Broken Links", markdown); // Should have broken links section
            Assert.Contains("429", markdown); // Should mention 429 status codes
            
            // Verify with snapshot testing like End2EndSpecs
            await Verify(markdown);
            
            await mockServer.StopAsync();
        }
        finally
        {
            if (Directory.Exists(testPagesDir))
                Directory.Delete(testPagesDir, true);
        }
    }
    
    [Fact]
    public void ShouldParseRetryAfterHeaderCorrectly()
    {
        // arrange
        var crawlConfig = new CrawlConfiguration(
            new AbsoluteUri(new Uri("https://example.com")), 
            5, TimeSpan.FromSeconds(5));
            
        var crawler = Sys.ActorOf(Props.Create(() => new CrawlerActor(crawlConfig, ActorRefs.Nobody)), "crawler");
        
        // Create a mock response message (this is more of a unit test for the logic)
        using var response = new HttpResponseMessage(HttpStatusCode.TooManyRequests);
        
        // Test seconds format
        response.Headers.Add("Retry-After", "30");
        // We can't directly test the private method, but we can verify the logic works through integration
        
        // Test date format  
        var futureTime = DateTimeOffset.UtcNow.AddMinutes(5);
        response.Headers.Clear();
        response.Headers.Add("Retry-After", futureTime.ToString("R"));
        
        // This test mainly verifies that the CrawlerActor can be instantiated with the new retry logic
        Assert.NotNull(crawler);
        _output.WriteLine("CrawlerActor with retry logic created successfully");
    }
    
    private IWebHost CreateMockRateLimitedServer(RetryAttemptTracker tracker, int port)
    {
        return new WebHostBuilder()
            .UseKestrel()
            .UseUrls($"http://127.0.0.1:{port}")
            .Configure(app =>
            {
                app.Run(async context =>
                {
                    var path = context.Request.Path.Value ?? "/";
                    var url = $"http://127.0.0.1:{port}{path}";
                    
                    var attemptCount = tracker.IncrementAttempt(url);
                    _output.WriteLine($"Request to {url} - Attempt #{attemptCount}");
                    
                    if (path == "/always-429")
                    {
                        // Always return 429
                        context.Response.StatusCode = 429;
                        await context.Response.WriteAsync("Too Many Requests - Always");
                    }
                    else if (path == "/retry-then-succeed")
                    {
                        // Return 429 for first 2 attempts, then succeed
                        if (attemptCount <= 2)
                        {
                            context.Response.StatusCode = 429;
                            await context.Response.WriteAsync("Too Many Requests - Retry");
                        }
                        else
                        {
                            context.Response.StatusCode = 200;
                            await context.Response.WriteAsync("Success after retries!");
                        }
                    }
                    else if (path == "/with-retry-after")
                    {
                        // Return 429 with Retry-After header for first attempt, then succeed
                        if (attemptCount <= 1)
                        {
                            context.Response.StatusCode = 429;
                            context.Response.Headers["Retry-After"] = "1"; // 1 second
                            await context.Response.WriteAsync("Too Many Requests - With Header");
                        }
                        else
                        {
                            context.Response.StatusCode = 200;
                            await context.Response.WriteAsync("Success after Retry-After!");
                        }
                    }
                    else if (path == "/another-rate-limited")
                    {
                        // Always return 429 to test multiple failing links
                        context.Response.StatusCode = 429;
                        await context.Response.WriteAsync("Too Many Requests - Another");
                    }
                    else
                    {
                        context.Response.StatusCode = 404;
                        await context.Response.WriteAsync("Not Found");
                    }
                });
            })
            .Build();
    }
}

public class RetryAttemptTracker
{
    private readonly ConcurrentDictionary<string, int> _attemptCounts = new();

    public int IncrementAttempt(string url)
    {
        return _attemptCounts.AddOrUpdate(url, 1, (_, count) => count + 1);
    }

    public Dictionary<string, int> GetAttemptCounts()
    {
        return _attemptCounts.ToDictionary(kvp => kvp.Key, kvp => kvp.Value);
    }
}